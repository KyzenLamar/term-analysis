import streamlit as st
import tempfile
import os
import pandas as pd
import re
import csv

from term_analysis_core import (
    load_terms_dictionary_csv,
    analyze_document_v2,
    get_unmatched_sentences,
    export_results_to_csv,
)
# from ml_semantic_search import load_wrong_usages, semantic_search

# ----------- BRANDING / DESIGN -------------
st.markdown("""
<style>
@font-face {
    font-family: 'Volja';
    src: url('volja-regular_w.ttf') format('truetype');
    font-weight: normal;
    font-style: normal;
}
.stApp, body {
    background-color: #F5F5F2;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
}
h1, h2, h3, .stMarkdown h1, .stMarkdown h2, .stMarkdown h3 {
    color: #6A653A;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
    font-weight: 700;
    letter-spacing: 0.03em;
}
.stButton > button {
    background-color: #6A653A;
    color: #fff;
    border-radius: 10px;
    font-weight: 600;
    font-size: 1em;
    padding: 0.5em 2em;
    border: none;
    transition: background 0.3s;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
}
.stButton > button:hover {
    background-color: #F39200;
    color: #232323;
}
.block-container, .css-18e3th9 {
    max-width: 70vw !important;
    padding-left: 2rem !important;
    padding-right: 2rem !important;
    border-radius: 16px;
    box-shadow: 0 2px 8px rgba(106,101,58, 0.08);
}
.stDataFrame, .stTable {
    background: #fff;
    border-radius: 10px;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
}
.stDataFrame thead tr th {
    background-color: #6A653A !important;
    color: #fff !important;
}
.stDataFrame tbody tr td {
    background-color: #fff !important;
    color: #232323 !important;
}
.highlight-term {
    background-color: #F39200;
    color: #232323 !important;
    border-radius: 6px;
    padding: 2px 4px;
    font-weight: bold;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
}
.footer {
    color: #6A653A;
    font-size: 0.9em;
    margin-top: 2em;
    text-align: center;
    font-family: 'Volja', Montserrat, Arial, sans-serif !important;
}
</style>
""", unsafe_allow_html=True)

# –õ–æ–≥–æ—Ç–∏–ø –°—É—Ö–æ–ø—É—Ç–Ω–∏—Ö –≤—ñ–π—Å—å–∫ –ó–°–£ (SVG –∞–±–æ PNG)
st.markdown(
    """
    <div style="display:flex; align-items:center; gap:1em;">
        <h1 style="margin-bottom:0;">‚ö°Ô∏è Term Analysis Tool ‚ö°Ô∏è</h1>
    </div>
    """,
    unsafe_allow_html=True
)

# ---------- END BRANDING ------------

# --- AI (—Å–µ–º–∞–Ω—Ç–∏–∫–∞) ‚Äî –ª–µ–¥–∞—á–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è (–∫–æ–ª–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á —É–≤—ñ–º–∫–Ω–µ)
from ml_semantic_search import load_wrong_usages, semantic_search

@st.cache_resource(show_spinner=False)
def load_ai_resources():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –º–æ–¥–µ–ª—å + –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ wrong_usage –æ–¥–∏–Ω —Ä–∞–∑, –∫–æ–ª–∏ AI —É–≤—ñ–º–∫–Ω–µ–Ω–æ.
    """
    wrong_csv_path = "all_generated_wrong_usages.csv"
    wrong_sentences, wrong_terms, wrong_comments, wrong_embeds = load_wrong_usages(wrong_csv_path)
    return wrong_sentences, wrong_terms, wrong_comments, wrong_embeds


def highlight_term_in_sentence(sentence, term):
    if not sentence or not term:
        return sentence
    pattern = r'\b{}\b'.format(re.escape(term))
    highlighted = re.sub(
        pattern,
        lambda m: f'<span class="highlight-term">{m.group(0)}</span>',
        sentence,
        flags=re.IGNORECASE
    )
    return highlighted

def save_terms_dictionary_csv(terms, file_path):
    fieldnames = ["approved_term", "synonyms", "category", "wrong_usages", "context_examples"]
    with open(file_path, "w", encoding="utf-8", newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for t in terms:
            writer.writerow({
                "approved_term": t["approved_term"],
                "synonyms": ";".join(t["synonyms"]),
                "category": t["category"],
                "wrong_usages": ";".join(t["wrong_usages"]),
                "context_examples": ";".join(t["context_examples"])
            })

# --- ML Semantic Search: Load model & data (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ!)
# @st.cache_resource
# def get_semantic_search_model():
#     wrong_csv_path = "all_generated_wrong_usages.csv"  # —à–ª—è—Ö –¥–æ —Ç–≤–æ–≥–æ CSV –∑ wrong_usage
#     wrong_sentences, wrong_terms, wrong_comments, wrong_embeds = load_wrong_usages(wrong_csv_path)
#     return wrong_sentences, wrong_terms, wrong_comments, wrong_embeds
#
# wrong_sentences, wrong_terms, wrong_comments, wrong_embeds = get_semantic_search_model()

# ---- 1. –ù–∞–≤—ñ–≥–∞—Ü—ñ—è –º—ñ–∂ —Å—Ç–æ—Ä—ñ–Ω–∫–∞–º–∏ ----
page = st.sidebar.radio("–°—Ç–æ—Ä—ñ–Ω–∫–∞", ["–ê–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤", "–†–µ–¥–∞–∫—Ç–æ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞"])

# ---- 2. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ session_state ----
if "dict_path" not in st.session_state:
    st.session_state["dict_path"] = None
if "dict_df" not in st.session_state:
    st.session_state["dict_df"] = None
if "doc_path" not in st.session_state:
    st.session_state["doc_path"] = None
if "doc_name" not in st.session_state:
    st.session_state["doc_name"] = None
if "results" not in st.session_state:
    st.session_state["results"] = None
if "unmatched_sentences" not in st.session_state:
    st.session_state["unmatched_sentences"] = None

# ---- 3. –ê–ù–ê–õ–Ü–ó –î–û–ö–£–ú–ï–ù–¢–Ü–í ----
if page == "–ê–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤":
    st.write(
        "–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å –∑–∞—Ç–≤–µ—Ä–¥–∂–µ–Ω–∏–º —Ç–µ—Ä–º—ñ–Ω–∞–º —Ç–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–∏–º –Ω–∞–∑–≤–∞–º. "
        "–ó–∞–≤–∞–Ω—Ç–∞–∂ .docx –∞–±–æ .pdf, –≤–∏–±–µ—Ä–∏ —Å–ª–æ–≤–Ω–∏–∫ ‚Äî –æ—Ç—Ä–∏–º–∞–π –º–∏—Ç—Ç—î–≤–∏–π –∑–≤—ñ—Ç –ø—Ä–æ –ø–æ–º–∏–ª–∫–∏ —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—ó!"
    )

    with st.sidebar:
        st.header("–ü–∞—Ä–∞–º–µ—Ç—Ä–∏")
        synonyms_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂ —Å–ª–æ–≤–Ω–∏–∫ —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤", type=["csv"])
        fuzzy_threshold = st.slider("–ü–æ—Ä—ñ–≥ fuzzy matching (–õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω)", 60, 100, 88)
        semantic_threshold = st.slider("–ü–æ—Ä—ñ–≥ Semantic Search (0.7-0.95)", 0.7, 0.95, 0.8, step=0.01)
        st.markdown("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** 88 ‚Äî –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –≤—ñ–π—Å—å–∫–æ–≤–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤")
        ai_enabled = st.toggle("–£–≤—ñ–º–∫–Ω—É—Ç–∏ AI-–ø–æ—à—É–∫ (–ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ)", value=False,
                               help="–°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –º–æ–¥–µ–ª—ñ –∑ Hugging Face. –ú–æ–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏—Å—è –¥–æ–≤—à–µ –Ω–∞ —Ö–æ–ª–æ–¥–Ω–æ–º—É —Å—Ç–∞—Ä—Ç—ñ.")

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è/–ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ª–æ–≤–Ω–∏–∫–∞
    if synonyms_file:
        temp_dict_path = f"uploaded_{synonyms_file.name}"
        with open(temp_dict_path, "wb") as f:
            f.write(synonyms_file.read())
        st.session_state["dict_path"] = temp_dict_path
        terms = load_terms_dictionary_csv(temp_dict_path)
        df_dict = pd.DataFrame([{
            "approved_term": t["approved_term"],
            "synonyms": "; ".join(t["synonyms"]),
            "category": t["category"],
            "wrong_usages": "; ".join(t["wrong_usages"]),
            "context_examples": "; ".join(t["context_examples"])
        } for t in terms])
        st.session_state["dict_df"] = df_dict
        st.success(f"–°–ª–æ–≤–Ω–∏–∫ –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {synonyms_file.name}")
    elif st.session_state["dict_path"] is not None:
        st.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Å–ª–æ–≤–Ω–∏–∫: {os.path.basename(st.session_state['dict_path'])}")

    # --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è/–ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
    uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂ –¥–æ–∫—É–º–µ–Ω—Ç (.docx –∞–±–æ .pdf)", type=["docx", "pdf"])
    if uploaded_file:
        temp_doc_path = f"uploaded_{uploaded_file.name}"
        with open(temp_doc_path, "wb") as f:
            f.write(uploaded_file.read())
        st.session_state["doc_path"] = temp_doc_path
        st.session_state["doc_name"] = uploaded_file.name
        st.success(f"–î–æ–∫—É–º–µ–Ω—Ç –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {uploaded_file.name}")
    elif st.session_state["doc_path"] is not None:
        st.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–æ–∫—É–º–µ–Ω—Ç: {st.session_state['doc_name']}")

    analyze_btn = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç–∏ –∞–Ω–∞–ª—ñ–∑", use_container_width=True)
    if analyze_btn:
        if st.session_state["doc_path"] is None or st.session_state["dict_path"] is None:
            st.error("–ó–∞–≤–∞–Ω—Ç–∞–∂ —ñ –¥–æ–∫—É–º–µ–Ω—Ç, —ñ —Å–ª–æ–≤–Ω–∏–∫!")
        else:
            with st.spinner("–ê–Ω–∞–ª—ñ–∑—É—î–º–æ –¥–æ–∫—É–º–µ–Ω—Ç..."):
                terms = load_terms_dictionary_csv(st.session_state["dict_path"])
                results = analyze_document_v2(st.session_state["doc_path"], terms, fuzzy_threshold=fuzzy_threshold)
                # get_unmatched_sentences –ø–æ–≤–∏–Ω–Ω–∞ –ø–æ–≤–µ—Ä—Ç–∞—Ç–∏ [{"page": ..., "sentence": ...}]
                unmatched_sentences = get_unmatched_sentences(st.session_state["doc_path"], terms, fuzzy_threshold)
                st.session_state["results"] = results
                st.session_state["unmatched_sentences"] = unmatched_sentences

    results = st.session_state.get("results", [])
    unmatched_sentences = st.session_state.get("unmatched_sentences", [])

    # --- Rule-based results
    if results:
        st.warning(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(results)} –≤–∏–ø–∞–¥–∫—ñ–≤ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Ç–µ—Ä–º—ñ–Ω—ñ–≤!")
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_csv:
            export_results_to_csv(results, tmp_csv.name)
            tmp_csv.flush()
            with open(tmp_csv.name, "rb") as f:
                st.download_button(
                    label="‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑–≤—ñ—Ç (.csv)",
                    data=f,
                    file_name="term_analysis_report.csv",
                    mime="text/csv",
                )
        st.subheader("–í–∏—è–≤–ª–µ–Ω—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∏ (–∑ –ø—ñ–¥—Å–≤—ñ—á–µ–Ω–Ω—è–º):")
        for i, row in df.iterrows():
            st.markdown(
                f'<b>–°—Ç–æ—Ä—ñ–Ω–∫–∞:</b> <span style="color:#6A653A;font-weight:bold;">{row.get("page", "-")}</span> | '
                f'<b>–¢–µ—Ä–º—ñ–Ω:</b> <span style="color:#1976d2">{row["found_term"]}</span> | '
                f'<b>–ö–æ–Ω—Ç–µ–∫—Å—Ç:</b> {highlight_term_in_sentence(row["context"], row["found_term"])}',
                unsafe_allow_html=True
            )

    # --- SEMANTIC SEARCH (AI –ø–æ—à—É–∫ –ø–æ–º–∏–ª–æ–∫) ---
    st.subheader("üîé –°–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ (AI –ø–æ—à—É–∫ –ø–æ–º–∏–ª–æ–∫)")
    if not ai_enabled:
        st.info(
            "AI-–ø–æ—à—É–∫ –≤–∏–º–∫–Ω–µ–Ω–æ. –£–≤—ñ–º–∫–Ω–∏ –ø–µ—Ä–µ–º–∏–∫–∞—á —É —Å–∞–π–¥–±–∞—Ä—ñ, —è–∫—â–æ —Ö–æ—á–µ—à –¥–æ–ø–æ–≤–Ω–∏—Ç–∏ rule-based –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–º –∞–Ω–∞–ª—ñ–∑–æ–º.")
    else:
        st.caption("‚ö†Ô∏è –ü–µ—Ä—à–∏–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ –±—É—Ç–∏ –¥–æ–≤—à–∏–º ‚Äî –º–æ–¥–µ–ª—å –∫–µ—à—É—î—Ç—å—Å—è.")
        if unmatched_sentences:
            with st.status(
                    "–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ AI-—Ä–µ—Å—É—Ä—Å–∏ (–º–æ–¥–µ–ª—å + –µ–º–±–µ–¥–¥–∏–Ω–≥–∏). –¶–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ 20‚Äì60 —Å–µ–∫. –Ω–∞ —Ö–æ–ª–æ–¥–Ω–æ–º—É —Å—Ç–∞—Ä—Ç—ñ‚Ä¶",
                    expanded=True):
                wrong_sentences, wrong_terms, wrong_comments, wrong_embeds = load_ai_resources()
            semantic_results = []
            with st.spinner("–ü—Ä–æ–≤–æ–¥–∏–º–æ —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑‚Ä¶"):
                for unmatched in unmatched_sentences:
                    sentence = unmatched["sentence"] if isinstance(unmatched, dict) else unmatched
                    page = unmatched["page"] if isinstance(unmatched, dict) else "-"
                    hits = semantic_search(
                        sentence,
                        wrong_sentences,
                        wrong_terms,
                        wrong_comments,
                        wrong_embeds,
                        topn=5,
                        threshold=semantic_threshold
                    )
                    if hits:
                        for match in hits:
                            semantic_results.append({
                                "page": page,
                                "sentence": sentence,
                                "approved_term": match["approved_term"],
                                "wrong_usage": match["wrong_usage"],
                                "comment": match["comment"],
                                "score": match["score"]
                            })
            if semantic_results:
                st.success(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(semantic_results)} –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫!")
                df_sem = pd.DataFrame(semantic_results)
                st.dataframe(df_sem, use_container_width=True)
                st.subheader("–í–∏—è–≤–ª–µ–Ω—ñ AI-–ø–æ–º–∏–ª–∫–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–∏):")
                for _, row in df_sem.iterrows():
                    st.markdown(
                        f'<b>–°—Ç–æ—Ä—ñ–Ω–∫–∞:</b> <span style="color:#6A653A;font-weight:bold;">{row.get("page", "-")}</span> | '
                        f'<b>–í—Ö—ñ–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è:</b> <span style="color:#1976d2">{row["sentence"]}</span><br>'
                        f'<b>–°—Ö–æ–∂–∏–π –ø—Ä–∏–∫–ª–∞–¥ –ø–æ–º–∏–ª–∫–∏:</b> <span class="highlight-term">{row["wrong_usage"]}</span> '
                        f'(<b>–¢–µ—Ä–º—ñ–Ω:</b> {row["approved_term"]}, <b>–°—Ö–æ–∂—ñ—Å—Ç—å:</b> <b>{row["score"]}</b>, <b>–ö–æ–º–µ–Ω—Ç–∞—Ä:</b> {row["comment"]})',
                        unsafe_allow_html=True
                    )
                # Download CSV
                csv_sem = df_sem.to_csv(index=False, encoding="utf-8")
                st.download_button(
                    label="‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ AI semantic –∑–≤—ñ—Ç (.csv)",
                    data=csv_sem,
                    file_name="ai_semantic_report.csv",
                    mime="text/csv"
                )
            else:
                st.info("–°–µ–º–∞–Ω—Ç–∏—á–Ω–∏—Ö –ø–æ–º–∏–ª–æ–∫ –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ –Ω–∞ –≤–∏–±—Ä–∞–Ω–æ–º—É –ø–æ—Ä–æ–∑—ñ.")

    # –û—á–∏—â–µ–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    if st.button("–û—á–∏—Å—Ç–∏—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"):
        st.session_state["results"] = None
        st.session_state["unmatched_sentences"] = None
        st.rerun()
    # –û—á–∏—â–µ–Ω–Ω—è –≤—Å—ñ—Ö —Ñ–∞–π–ª—ñ–≤ —ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    if st.button("–û—á–∏—Å—Ç–∏—Ç–∏ –≤—Å–µ (—Å–ª–æ–≤–Ω–∏–∫, –¥–æ–∫—É–º–µ–Ω—Ç, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)"):
        st.session_state["dict_path"] = None
        st.session_state["dict_df"] = None
        st.session_state["doc_path"] = None
        st.session_state["doc_name"] = None
        st.session_state["results"] = None
        st.session_state["unmatched_sentences"] = None
        st.rerun()

    st.markdown("---")
    st.markdown(
        """
        <div class="footer">
        –ó–∞—Å—Ç–æ—Å—É–Ω–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–æ –¥–ª—è –°—É—Ö–æ–ø—É—Ç–Ω–∏—Ö –≤—ñ–π—Å—å–∫ –ó–±—Ä–æ–π–Ω–∏—Ö –°–∏–ª –£–∫—Ä–∞—ó–Ω–∏.<br>
        Made with üíôüíõ by [–¢–≤—ñ–π –ø–æ–º—ñ—á–Ω–∏–∫ Igorüßîüèª‚Äç]
        </div>
        """,
        unsafe_allow_html=True
    )

# ---- 4. –†–ï–î–ê–ö–¢–û–† –°–õ–û–í–ù–ò–ö–ê ----
elif page == "–†–µ–¥–∞–∫—Ç–æ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞":
    st.header("üìñ –†–µ–¥–∞–∫—Ç–æ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞ —Ç–µ—Ä–º—ñ–Ω—ñ–≤")

    if st.session_state.get("dict_df") is not None:
        df_dict = st.session_state["dict_df"]
        dict_path = st.session_state["dict_path"]
        st.success(f"–°–ª–æ–≤–Ω–∏–∫ –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ –ê–Ω–∞–ª—ñ–∑—É: {os.path.basename(dict_path)}")
    else:
        dict_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂ —Å–ª–æ–≤–Ω–∏–∫ —Ç–µ—Ä–º—ñ–Ω—ñ–≤ (CSV)", type=["csv"])
        dict_path = None
        df_dict = None
        if dict_file:
            dict_path = f"uploaded_{dict_file.name}"
            with open(dict_path, "wb") as f:
                f.write(dict_file.read())
            terms = load_terms_dictionary_csv(dict_path)
            df_dict = pd.DataFrame([{
                "approved_term": t["approved_term"],
                "synonyms": "; ".join(t["synonyms"]),
                "category": t["category"],
                "wrong_usages": "; ".join(t["wrong_usages"]),
                "context_examples": "; ".join(t["context_examples"])
            } for t in terms])
            st.session_state["dict_df"] = df_dict
            st.session_state["dict_path"] = dict_path

    if st.session_state.get("dict_df") is not None:
        st.subheader("–†–µ–¥–∞–≥—É–π —Å–ª–æ–≤–Ω–∏–∫ –ø—Ä—è–º–æ —É —Ç–∞–±–ª–∏—Ü—ñ:")
        edited_df = st.data_editor(
            st.session_state["dict_df"],
            num_rows="dynamic",
            key="edit_dict"
        )
        if st.button("üíæ –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–º—ñ–Ω–∏ —É CSV"):
            terms = []
            for i, row in edited_df.iterrows():
                terms.append({
                    "approved_term": str(row["approved_term"]).strip(),
                    "synonyms": [s.strip() for s in str(row["synonyms"]).split(';') if s.strip()],
                    "category": str(row.get("category", "")).strip(),
                    "wrong_usages": [s.strip() for s in str(row.get("wrong_usages", "")).split(';') if s.strip()],
                    "context_examples": [s.strip() for s in str(row.get("context_examples", "")).split(';') if s.strip()],
                })
            save_terms_dictionary_csv(terms, st.session_state["dict_path"])
            st.session_state["dict_df"] = edited_df
            st.success("–ó–º—ñ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É CSV!")

        st.markdown("---")
        st.subheader("–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤–Ω–∏–∫:")
        with open(st.session_state["dict_path"], "rb") as f:
            st.download_button(
                label="‚¨áÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV",
                data=f,
                file_name=f"updated_{os.path.basename(st.session_state['dict_path'])}",
                mime="text/csv",
            )